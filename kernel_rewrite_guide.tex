\subsection{Workflow}

What follows is a tutorial to using the library, to give you an idea of how
general the necessary modifications are.

We followed a similar path to that of the OP-DSL family, only our ``DSL'' is so
simple that it can be implemented using a few macro functions, and we did not
use a source to source translator.

Supplying the algorithm to the library is done by modifying the
\lstinline{skeleton.hpp} and \lstinline{skeleton_func.hpp} files in the
\lstinline{kernels} directory. All coding is done in the C++ language.

\subsubsection{User function}

The body of the parallel loop: the user function is defined using the
\lstinline!USER_FUNCTION_SIGNATURE! macro in the modified
\lstinline{skeleton_func.hpp} file. It can then call further inline functions.

It will be passed pointers to the first indirect and direct data it accesses
and also the corresponding two strides. This way, it can for example access the
\lstinline{i}th component of the first indirect data point by calling
\lstinline!indirect_data1[i * indirect_stride]!.

The address of the output is also supplied, this is where the calculated
indirect increment is placed. Only the increments are returned to allow
controlled write-back of the results in different parallelisation methods.

All of the pointers should be marked with the \lstinline!RESTRICT! macro that
signals to the compiler that there is no overlap between the data accessed
through these pointers.

\subsubsection{Parallelisation administration}

Before calling the user function, some administration overhead is needed, such
as typecasting and extracting pointers from arrays, calculating the iteration
variable and caching; this is implemented by modifying the
\lstinline{skeleton_func.hpp}.

The name of the file, the header guard and the enclosing namespace should be
modified to something more descriptive of the application.

It is good practice to extract constants such as the dimension of the mesh and
the number of components in the indirect and direct data points, these should be
defined at the top of the namespace.

The different parallelisation methods (different kernels) are defined in
different structs, all having one static \lstinline!call! method that will be
called by the library. This has a boolean template parameter that is
\lstinline!true! if the indirect data is in Struct of Arrays layout (see Section
\ref{aos-to-soa}). The direct data is always in SoA layout.

This method has arguments for the indirect input, indirect output, the directly
accessed data points, the mappings. These pointers are of type
\lstinline!void**! (array of arrays structure), except for the mappings that are
\lstinline!std::uint32_t!. The different arrays should be typecast to the
appropriate types. There should also be a local array for the increments.

Iteration is based on the iteration variable (loop variable): in sequential
kernels, this is supplied as a parameter, in GPU kernels (where it is the thread
id), it is supplied by the CUDA runtime. In the latter case, the code should
also check whether the current thread id is within the loop boundary.

The hierarchical kernel (the kernel that uses the hierarchical colouring method
and caches into shared memory) has also as parameters the list of indirect data
points to be cached (in Compressed Sparse Row format), the colours of the
threads, the number of thread colours and the offsets of the blocks (because the
variable block size).

The block offsets should be used to calculate the iteration variable, so instead
of: \lstinline!blockDim.x * blockIdx.x + threadIdx.x!, it becomes
\lstinline!block_offsets[blockIdx.x] + threadIdx.x!.

The skeleton for the cache-in and cache-out code is composed of nested
conditionals that depend on the type of the data. Since the user knows the type,
the structure can be simplified by removing any unneeded branch.

The computation code is similar to the sequential and global kernels, only the
write-back of the increments is done in two steps: first the shared memory is
cleared, then, in a for loop iterating over the thread colours, the shared
memory is updated by the increments. This is then written back to global memory.

\subsubsection{Using the library}

After the implementation of the algorithm, the library will take care of the
parallelisation.

The data and mappings is read from specified input streams. To call the loop,
the user calls on of the \lstinline!loopCPUCellCentred!,
\lstinline!loopCPUCellCentredOMP!, \lstinline!loopGPUCellCentred!,
\lstinline!loopGPUHierarchical! methods of the \lstinline!Problem! class. The
have a template parameter that is the struct implementing the kernel code.
