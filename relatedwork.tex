\noindent
Algorithms defined on unstructured meshes form an important 
class of applications at many organizations. It is one of the seven dwarfs -- 
common computation-communication patterns or motifs occurring in parallel 
numerical applications -- identified by Colella in 2004~\cite{Colella2004}. 
Discretizations such as finite volumes (FV) or finite elements (FE) often rely 
on these meshes to deliver high-quality results. Indeed there is a large number 
of papers detailing such algorithms, and a wide range of commercial, government, 
and academic research codes (e.g. OpenFOAM~\cite{OpenFoamUserGuide}, Rolls-Royce 
Hydra~\cite{moinier2002edge}, FUN3D~\cite{biedron2017fun3d}). All such 
applications use unstructured meshes in some shape or form, and are often used 
for large experiments, consisting of millions or even billions of mesh elements. 
These codes are often critical to production and consume large portions of 
high-performance computing systems time. As such, the efficient execution of 
these applications on the parallel architectures of the day has been and 
continues to be crucial to the organizations and stake-holders that have 
invested in them for continued scientific delivery. Over the years, many works 
have discussed and presented techniques for efficient implementations, initially 
focusing on traditional CPU architectures~\cite{mavriplis2002parallel, 
jin1999openmp}, then many-core processors such as GPUs, and 
even architectures such as FPGAs~\cite{nagy2014accelerating, 
akamine2012reconfigurable}. Many libraries have also been developed targeting 
unstructured-mesh solvers, from classical libraries~\cite{trilinos, PETSc} to domain specific 
languages~\cite{devito2011liszt, giles2012op2, pyfr2016}. 

The adoption of GPUs for these kind of computations has already led to 
considerable speedups over traditional CPU architectures due to the 
massive parallelism available on GPUs~\cite{Reguly2015, ELSEN200810148, 
cohen2009fast}. Other notable works have further looked at improving 
performance. Remacle et al. \cite{remacle2016gpu}, explores efficiently solving 
elliptic problems on unstructured hexahedral meshes on GPUs. They use shared 
memory for improving data locality, but advanced techniques, such as reordering 
and partitioning are not utilized. Work done by Castro et al. 
\cite{shallow_water} on implementing path-conservative Roe type high-order 
finite volume schemes to simulate shallow flows uses auxiliary accumulators to 
avoid data races while indirectly incrementing. Wu et al. 
\cite{wu2013complexity} introduce caching using the shared memory with 
partitioning (clustering), but do not use coloring. Instead they use a 
duplication method similar to that of LULESH and miniAero, as described below. 
Fu et al. \cite{fu2014architecting} also create contiguous patches (blocks) in 
the mesh to be loaded into shared memory, although they partition the nodes (the 
to-set) but not the elements (the from-set of the mapping). Furthermore, they do 
not load all data into shared memory, only what is inside the patch. Writing the 
result back to shared memory is done by a binary search for the column index and 
atomic adds, which leads to inefficiencies on the GPU. 

Parallel to the above work, the US Department of Energy labs have released a 
set of proxy applications that represent large internal production codes,
showing some of the computational and algorithmic challenges to be overcome on 
novel and emerging architectures Lulesh \cite{LULESH2:changes}, 
miniAero~\cite{miniaero}, BookLeaf~\cite{bookleaf}, MiniFE~\cite{minife}, 
PENNANT~\cite{pennant}. Out of this suite of codes there are three key 
approaches to handling data races: (1) allocate large temporary arrays where 
the intermediate results (i.e. the increments) are placed, avoiding any race 
conditions, followed by the use of a separate kernel to gather the results, 
(2) use atomics, (3) use coloring. These all lead to increased warp divergence 
and high data access latencies on GPUs; and the use of the temporary array also 
leads to more data allocations and movement, further constraining bandwidth. 

The research detailed in the present work is based on previous work 
in~\cite{op2}, where the OP2 library's GPU parallelization use shared memory on 
GPUs using CUDA for caching with a two level ``hierarchical'' coloring. However, 
we demonstrate superior execution strategies on GPUs with reordering of threads 
and data, to increase data reuse and maximize data locality. Instead of directly 
porting a specific application to use these techniques we present our methods 
as a classical library that can be used in general to accelerate unstructured 
mesh applications, and in particular the indirect increment algorithmic 
pattern, on GPUs.

Most applications of interest for our work implements finite volume algorithms, 
and low order finite element algorithms, which has a lower computational 
intensity compared to the number of memory transactions. Thus our optimizations 
are targeted to avoid data movement, exploiting locality. In contrast high 
order finite element methods usually have significantly higher computational 
intensity, where here is a higher number of computations per data element 
accessed that can hide the cost of the memory access. While our techniques could 
potentially improve locality, memory bandwidth is less of a concern for such 
applications.


% In most finite volume algorithms, and low order finite element algorithms the 
% ratio of computations to number of bytes is relatively low - at least 
% compared to the ideal balance on modern GPUs. Therefore the proper usage of 
% the memory system for such simulations is crucial to get good performance. 
% High order finite elements usually have much higher computational 
% requirements, thus memory bandwidth is less of a concern. The throughput of 
% memory accesses is the main bottleneck for a large class of applications, thus 
% our goal is to lower the impact of the memory transactions. Since in most 
% applications of interest, there isn't enough computations to hide the cost of 
% memory movement, we can either increase the number of memory transactions in 
% flight (to more efficiently utilise bandwidth), or decrease the number of 
% memory transactions. To achieve the latter goal, a common technique is the use 
% of shared memory within CUDA thread blocks as an explicitly managed cache, 
% because it has much higher bandwidth and lower latency than global memory. The 
% challenge then is to maximise data re-use within shared memory, and optimise 
% access patterns to both shared and global memory.
