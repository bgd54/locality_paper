\section{Introduction}\label{introduction}

\input{introduction}

\section{Parallelisation on GPUs}\label{parallelisation-on-gpu}

\input{theory}

\section{Measurements}\label{measurements}

\subsection{Used applications}\label{used-applications}
\input{applications}

\subsection{Experimental setup}\label{experimental-setup}
For testing we used NVIDIA Tesla P100 and V100 GPUs, Intel(R) Xeon(R) CPU
E5-1660 (3.20GHz base frequency, 1 socket with 8 cores) with Ubuntu 16.04. We
used the nvcc compiler with CUDA 9.0 (V9.0.176). The parameters of the GPUs
are shown in Table \ref{tab:GPU_datasheet}. We discuss the results on the
P100 in detail, however, the results were similar on the newer architecture.
\begin{table}
\centering
\begin{tabular}{|l|c|c|}
\hline
  & P100 (GP100) & V100 (GV100)\\ \hline
  Streaming Multiprocessors (SM) 		& 56	& 80\\ \hline
  Max Thread Block Size				& 1024	& 1024 \\ \hline
  Max Warps / Multiprocessor 			& 64 & 64	\\ \hline
  Max Threads / Multiprocessor		& 2048 & 2048	\\ \hline
  Max Thread Blocks / Multiprocessor 	& 32 & 32	\\ \hline
  Max Registers / Thread& 255 & 255	\\ \hline
  Shared Memory Size / SM	& 64 KB & 96 KB	\\ \hline
  Max 32 - bit Registers / SM			& 65536 & 65536\\ \hline
  Memory Size							& 16 GB	& 16 GB\\ \hline
  L2 Cache Size						& 4096 KB & 6144 KB\\ \hline
\end{tabular}
  \caption{Important informations about the NVIDIA Tesla P100 and V100 GPUs
  \cite{Pascal_whitepaper, Volta_whitepaper}}
\label{tab:GPU_datasheet}
\end{table}

There are three key factors that we study: (1) colouring approach, which may either be global or hierarchical, (2) method of data reordering, which may be none, GPS-based, or graph partitioning-based, (3) data layout, which may be AoS or SoA. All combinations of these are evaluated and compared to each other and the reference implementation.

When comparing performance of different versions, we used the achieved bandwidth
that is the key performance metric with memory-bound applications such as our
test applications. It is calculated by the following formula: $$\frac{\sum_{d}
w_dS_d}{T} \cdot I,$$ where $d$ iterates over the datasets, $w_d$ is $2$ if the
data is read and written, $1$ otherwise, $S_d$ is the size of the dataset (in
bytes), $T$ is the overall runtime of the kernel and $I$ is the number of
iterations.

We also collected other relevant metrics that describe the observations, such as 
\begin{itemize}
  \item data reuse factor (the average number of times an indirectly accessed
    data point is accessed),
  \item the number of read/write transactions from/to global memory, which is
    closely related to the data reuse factor but is affected by memory access
    patterns, and therefore cache line utilisation,
  \item the occupancy reflecting the number of threads resident on the SM versus
    the maximum - the higher this is, the better chance of hiding the latency of
    compute/memory operations and synchronisation
  \item the percentage of stalls occurring because of data requests, execution
    dependencies, or synchronisation,
   \item the number of block colours; the higher it is, the less work in a
     single kernel launch, which tends to lead to lower utilisation of the GPU,
  \item the number of thread colours; the higher this is the more
    synchronisations are required to apply the increments in shared memory ---
    but also strongly correlates with data reuse,
  \item warp execution efficiency (ratio of the average active threads per warp
    to the maximum number of threads per warp).
\end{itemize}
Studying performance and these metrics help us understand and explain why
certain variants are better than others.

\subsection{Measurement results}\label{measurement-results}

\input{results}

\section{Conclusion}\label{conclusion}

We investigated methods of accelerating parallel scientific computations on
unstructured meshes, specifically looking at improving the performance of
kernels with indirect increment access patterns.

We considered a number of well-known techniques, such as AoS and SoA, graph bandwidth minimising algorithms, global and hierarchical colouring approaches. We designed a reordering algorithm which uses k-way recursive partitioning to
improve data reuse within CUDA thread blocks.

We implemented a library that can automatically (without major modifications
from the part of the user) parallelise serial user code, avoiding data races
using global and hierarchical colouring. It uses optimisations specifically
targeting GPUs, such as caching in shared memory, reordering by colours to
reduce warp divergence and using vector types to more efficiently utilise
available global memory bandwidth.

Using this library, we analysed the performance of our algorithms on a number of
representative unstructured mesh applications varying a number of parameters,
such as the different thread block sizes and data layouts (Array of Structures
versus Structure of Arrays).

When comparing the performance of global and hierarchical colouring (shared
memory caching) approach, the shared memory approach consistently performed
better, since it could exploit the temporal locality in indirectly accessed data
by avoiding data races in shared memory with synchronisation within thread
blocks rather than different kernel launches.

We also analysed the performance of reordering based on GPS renumbering and
partitioning. The former improves global colouring with increasing spatial
reuse, while the latter can significantly improve the shared memory approach by
increasing data reuse within thread blocks, which results in smaller shared
memory and fewer global memory transactions.

We have shown that there is a trade-off between high data reuse and large
numbers of thread colours in hierarchical colouring that is especially
pronounced in 3D applications, and when the achieved occupancy is low: the more thread colours a block
has, the more synchronisations it will need, the latency of which can be hard to
hide when there are few eligible warps.

Using our methods, we were able to achieve performance gains of $10\%$
(Airfoil), $20\%$ (Volna), $75\%$ (Bookleaf), $75\%$ (Lulesh) and $25\%$
(miniAero) over the original implementations. These results significantly
advance the state of the art, demonstrating that the algorithmic patterns used
in most current implementations (particularly in case of US DoE codes
represented by LULESH and MiniAero) could be significantly improved upon by the
adoption of two-level colouring schemes and partitioning for increased data
reuse.

When carrying out this work, it had become clear that partitioning algorithms in
traditional libraries such as Metis and Scotch were not particularly well suited
for producing such small partition sizes. As potential future work, we wish to
explore algorithms that are better optimised for this purpose. The performance
of these partitioning algorithms was also low - parallelising this could be
another interesting challenge. Finally, we are planning to integrate these
algorithms into the OP2 library, so they can be automatically deployed on
applications that already use the OP2 library, such as Airfoil, BookLeaf, Volna
or Rolls-Royce Hydra.

% vim:set et sw=2 ts=2 tw=80:
