Deploying the locality exploiting algorithms developed in this research to 
existing and new unstructured-mesh applications is the subject of this appendix. 
The algorithms have been implemented as an open source library 
at~\cite{opt-library}. We first provide an overview of the re-engineering /
development process required to utilize the library followed by a step-by-step 
guide to the workflow involved. 

There are two primary concerns for deployment. First, given an existing 
application, its current data layout may need to change to accommodate AoS, SoA 
or reordering of mesh elements. Data layout reorganization has a one-off cost, and needs to be 
performed during initialization. Changing data-layouts between different 
computational kernels is usually prohibitively expensive and therefore avoided. 
Reordering will not affect direct kernels, but it will affect indirect kernels. 
AoS/SoA transformations affect all kernels. The impact of these transformations 
need to be considered as a whole on the applications. 

Secondly, the CUDA kernels that perform the computations need to be changed to 
load, process and store data as was described in 
Section~\ref{shared-memory-approach}. This will essentially involve directly 
modifying the computational kernels of the unstructured-mesh application we are 
deploying the optimizations to. The goal therefore is to minimize the amount 
of changes required to be done on the original code.

\textbf{Done up to here - Gihan}

% involve surrounding the computational kernel with code provided by our library. 

% \subsection{Integration into codes}

% There are two key components to deploying these transformations and 
% optimizations. First, the existing data layouts may need to be changed to
% accommodate AoS, SoA or reordering optimizations, and any auxiliary datasets
% need to be computed (e.g. exact block sizes when using partitioning-based
% reordering). Second, the CUDA kernels that perform the computations need to be
% changed to load, process and store data. The latter was described in Section
% \ref{shared-memory-approach} -- the computational kernel needs to be 
% surrounded by code provided by our library. Data reorganization and execution 
% planning has a one-off cost, and needs to be performed during 
% initialization---our library
% also provides these as functions. It can of course affect other parts of the
% code as well---as discussed above, reordering will not affect direct kernels,
% but it will affect indirect kernels, and AoS/SoA transformations affect all
% kernels. The impact of these transformations need to be considered as a whole 
% on the applications, considering re-organization between different 
% computational kernels is usually prohibitively expensive.


% In our library, the AoS--SoA transformation is governed by a tunable
% hyperparameter on a dataset-by-dataset basis. A hybrid SoA-AoS layout was not 
% considered, due to the fact that the library is designed to be integrated 
% into existing applicaitons, and the complexity


\subsection{User kernels}




The concept of a computation is in the form of a loop (or kernel) body. The loop
body is written by the user from a skeleton code, the library only calls it. The
same code can be used as in the serial algorithm, with the restrictions
described in Section \ref{unstructured-meshes} and with the modification that,
since the data layout is not necessarily the usual AoS, the accesses need to use
the stride parameters (supplied to the kernel by the library) that are defined
to be the distance between two consecutive data component (e.g. this will be $1$
in the case of AoS).

Four types of loops are supported: one serial and three parallel. The
parallelisations are done by (1) OpenMP on the CPU, (2) CUDA with global
colouring and (3) CUDA using the shared memory approach with hierarchical
colouring on the GPU. The CPU versions are not optimised and are only there for
testing and verification purposes. The user of the library can choose which of
these to apply.

The user kernel consists of two main levels. In the first, the pointers to the
data arrays are acquired, typecast and the result is written back into global
(GPU or CPU) memory. This is similar in the differently parallelised loops (e.g.
there is usually no difference between the serial and the OpenMP versions), but
there are small variations in the use of GPU shared memory, the synchronisation
steps and the calculation of the loop variable. (By loop variable, we mean the
index from the from-set.) In our current implementation, the user creates this
level from provided templates, but this can be easily automated by means of a
source-to-source translator tool. The second level is the calculation itself
that should be the same for all loop forms.

The data will be automatically copied to the device before the beginning of the
loop when the program is running on the GPU, and the result will be copied back
to the host after. The pointers supplied to the user kernel also point to the
location in the appropriate address space.

Data accessed only directly is always in SoA form, while the layout of the
data indirectly accessed is a user supplied compile time parameter. The layout
of the shared memory is determined by the user; it is SoA in the kernels in our
measurements. The layout of the mappings is AoS for all the parallelisation
methods except hierarchical colouring.

In the case of OpenMP and global colouring, the parallel execution is
synchronised by the library using multiple kernel launches. The hierarchical
colouring has some additional synchronisation steps, as described by Algorithm
\ref{code:shared} in Section \ref{shared-memory-approach}.

\subsection{Execution planning}

We use colouring to avoid data races. Two kinds of colouring algorithms are
used: global colouring is used for the OpenMP and the first CUDA
parallelisation, while hierarchical or two-layer colouring is used for the
shared memory approach.

The global colouring is a direct generalisation of the greedy graph colouring
algorithm: for all elements of the from-set, its colour will be one of those
that are used but none of its corresponding indirect points have.

The threads with different colours are started in different kernel launches. The
mappings and the direct accessed data arrays are reordered according to their
colour, so there is no need for another indirection.

If partitioning is used in the preprocessing step, the hierarchical colouring
reorders the threads according to that and maps the threads to blocks. The block
sizes are limited by a parameter (given by the user), and if the partitioning
algorithm produced a block that is larger, it is divided into two CUDA blocks.

The same greedy colouring algorithm described above is then used to colour the
thread blocks. Within the blocks we use the heuristics of ordering the threads
by removing the one with the minimum degree, placing it last in the ordering,
then recursively ordering the other threads.

After colouring the threads are sorted according to their colour to reduce warp
divergence, as described in Section \ref{optimisations}.

The mappings and the direct accessed data are transformed into SoA layout to
increase spatial locality. Similarly to the global colouring, these are then
reordered so they can be directly indexed by the loop variable.

\subsubsection{Reordering}

Before the execution of the loop an optional preprocessing step is to either
reorder the from- or to-set elements using the GPS algorithm, or map the threads
to blocks by a partitioning algorithm described in Section
\ref{increasing-data-reuse}.

We use the Scotch library\cite{scotch} for the GPS algorithm, and the
METIS\cite{metis} library for the multilevel k-way partitioning algorithm (using
64 bit integers for indices). We also tried the recursive bisection algorithm in
the METIS library, but the result was significantly worse, as well as the
partitioning algorithm in the Scotch library, but that failed to stay within
tolerance (and half of the blocks in the partition were empty, others were
larger then requested).

% vim:set et sts=2 sw=2 ts=2 tw=80:
