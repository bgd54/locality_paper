When implementing the library, the goal was to minimize the need for the users
to modify their computation algorithms.

\subsection{User kernels}

The concept of a computation is in the form of a loop (or kernel) body. The loop
body is written by the user from a skeleton code, the library only calls it. The
same code can be used as in the serial algorithm, with the restrictions
described in Section \ref{unstructured-meshes} and with the modification that,
since the data layout is not necessarily the usual AoS, the accesses need to use
the stride parameters (supplied to the kernel by the library) that are defined
to be the distance between two consecutive data component (e.g. this will be $1$
in the case of AoS).

Four types of loops are supported: one serial and three parallel. The
parallelisations are done by (1) OpenMP on the CPU, (2) CUDA with global
colouring and (3) CUDA using the shared memory approach with hierarchical
colouring on the GPU. The CPU versions are not optimised and are only there for
testing and verification purposes. The user of the library can choose which of
these to apply.

The user kernel consists of two main levels. In the first, the pointers to the
data arrays are acquired, typecast and the result is written back into global
(GPU or CPU) memory. This is similar in the differently parallelised loops (e.g.
there is usually no difference between the serial and the OpenMP versions), but
there are small variations in the use of GPU shared memory, the synchronisation
steps and the calculation of the loop variable. (By loop variable, we mean the
index from the from-set.) In our current implementation, the user creates this
level from provided templates, but this can be easily automated by means of a
source-to-source translator tool. The second level is the calculation itself
that should be the same for all loop forms.

The data will be automatically copied to the device before the beginning of the
loop when the program is running on the GPU, and the result will be copied back
to the host after. The pointers supplied to the user kernel also point to the
location in the appropriate address space.

Data accessed only directly is always in SoA form, while the layout of the
data indirectly accessed is a user supplied compile time parameter. The layout
of the shared memory is determined by the user; it is SoA in the kernels in our
measurements. The layout of the mappings is AoS for all the parallelisation
methods except hierarchical colouring.

In the case of OpenMP and global colouring, the parallel execution is
synchronised by the library using multiple kernel launches. The hierarchical
colouring has some additional synchronisation steps, as described by Algorithm
\ref{code:shared} in Section \ref{shared-memory-approach}.

\subsection{Execution planning}

We use colouring to avoid data races. Two kinds of colouring algorithms are
used: global colouring is used for the OpenMP and the first CUDA
parallelisation, while hierarchical or two-layer colouring is used for the
shared memory approach.

The global colouring is a direct generalisation of the greedy graph colouring
algorithm: for all elements of the from-set, its colour will be one of those
that are used but none of its corresponding indirect points have.

The threads with different colours are started in different kernel launches. The
mappings and the direct accessed data arrays are reordered according to their
colour, so there is no need for another indirection.

If partitioning is used in the preprocessing step, the hierarchical colouring
reorders the threads according to that and maps the threads to blocks. The block
sizes are limited by a parameter (given by the user), and if the partitioning
algorithm produced a block that is larger, it is divided into two CUDA blocks.

The same greedy colouring algorithm described above is then used to colour the
thread blocks. Within the blocks we use the heuristics of ordering the threads
by removing the one with the minimum degree, placing it last in the ordering,
then recursively ordering the other threads.

After colouring the threads are sorted according to their colour to reduce warp
divergence, as described in Section \ref{optimisations}.

The mappings and the direct accessed data are transformed into SoA layout to
increase spatial locality. Similarly to the global colouring, these are then
reordered so they can be directly indexed by the loop variable.

\subsubsection{Reordering}

Before the execution of the loop an optional preprocessing step is to either
reorder the from- or to-set elements using the GPS algorithm, or map the threads
to blocks by a partitioning algorithm described in Section
\ref{increasing-data-reuse}.

We use the Scotch library\cite{scotch} for the GPS algorithm, and the
METIS\cite{metis} library for the multilevel k-way partitioning algorithm (using
64 bit integers for indices). We also tried the recursive bisection algorithm in
the METIS library, but the result was significantly worse, as well as the
partitioning algorithm in the Scotch library, but that failed to stay within
tolerance (and half of the blocks in the partition were empty, others were
larger then requested).

It must be noted that these algorithms were developed for distributing workloads
on computing clusters that typically have much larger block size to total size
ratio. This also caused the reordering (partitioning) phase to be quite long:
even minutes; but this is a one-off cost: the reordering can be saved from the
library and reused many times later. Further improvement could be achieved by
using the parallel versions of the METIS library: ParMETIS\cite{parmetis} and
the alpha version mt-Metis\cite{mtmetis} libraries. Partitioning algorithms
targeting specifically small partition sizes required for CUDA thread blocks are
a target of future research.

% vim:set et sts=2 sw=2 ts=2 tw=80:
